# Проект асинхронного парсера на базе фреймворка scrapy

## Задачи парсера
Обработать сайт https://peps.python.org/ предложений по улучшению языка Python (PEP) и собрать следующие данные:
- Список всех PEP и их статусы;
- Статистические данные по статусам PEP (статус, количество).

Данные сохранить в файлы:
- pep_дата_время.csv - список PEP;
Файла имеет 3 колонки: "Номер", "Название", "Статус".
- status_summary_время_дата.csv - количество по статусам;
Файл имеет 2 колонки: "Статус", "Количество". В конце подведен итог: суммарное количество статусов.

## Инструменты
Для парсинга страниц использовался фреймворк scrapy. Были задействованы:
- spiders;
- items;
- feeds;
- piplines;
- csv.

## Как это работает
- Паук "pep" собирает с главной страницы все ссылки на PEP;
- Паук "pep" для каждой страницы PEP парсит данные о названии, номере и статусе, формирует Items;
- FEEDS сохраняет в csv файл данные о PEP, полученые пауком;
- Pipline ведет подсчет статистики по статусам PEP и сохраняет данные в csv файл при помощи стандартной библиотеки csv.

## Запуск парсера
- Клонируйте репозиторий на свой компьютер;
- Установите и настройте виртуальное окружение из файла requirements.txt;
- Из директории проекта запустите парсер командой:
```scrapy crawl pep```
- После успешного завершения работы парсера в диреткории ```results``` появятся два файл с результатом парсинга.
